{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlassner/dsml4220_lab10/blob/main/vl_lab10_agents_and_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCMZVm4kH-AF"
      },
      "source": [
        "# Lab 10: A simple Agent with Tools\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/DSML4220/blob/main/lab10_agents_and_tools.ipynb)\n",
        "\n",
        "In this lab we will use Ollama to create a simple agent armed with tools in order to help carry out tasks on our behalf. This notebook is based on the short blog posts/tutorials found [here](https://www.cohorte.co/blog/using-ollama-with-python-step-by-step-guide) and [here](https://towardsdatascience.com/ai-agents-from-zero-to-hero-part-1/).\n",
        "\n",
        "\n",
        "### Lab 10 Assignment/Task\n",
        "There are a few questions below that require some additional code to be written so that your agent can carry out other operations besides just addition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL23DdBOOP70"
      },
      "source": [
        "Let's start out by setting up Ollama to run in Colab. If you run this notebook locally and already have Ollama running, then you can skip these steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7HUh1vMOP70"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41IuM4xHO31c"
      },
      "source": [
        "Next we'll start a terminal from within Colab. Once the terminal is running, you will need to run the two commands, but at separate times.\n",
        "\n",
        "After you have run `%xterm` and the terminal opens, copy and paste the following line into the terminal and press enter:\n",
        "* `curl https://ollama.ai/install.sh | sh`\n",
        "\n",
        "After that has run, then start the Ollama server and have it run in the background by copying and pasting the following line into the terminal and pressing enter:\n",
        "* ` ollama serve &`\n",
        "\n",
        "(_Note: you may need to press enter one more time after `ollama serve` to see the terminal prompt again_)\n",
        "\n",
        "Next we will pull the small (1B parameter) Llama3.2 model down by running the following in the terminal:\n",
        "* `ollama pull llama3.2:1b`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owD9uV6KOjKo"
      },
      "outputs": [],
      "source": [
        "%xterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2fOHksIOP71"
      },
      "source": [
        "Now that Ollama is running, we can get started. The only module/library needed for this is the ollama python module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqJGyyiMQMW-"
      },
      "outputs": [],
      "source": [
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKCJxsS6hRNy"
      },
      "outputs": [],
      "source": [
        "import ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJGCcDwJ8hnR"
      },
      "source": [
        "Now, let's define a __tool__ for the agent/model to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuKjlanSOP71"
      },
      "outputs": [],
      "source": [
        "# Tool function to add two numbers\n",
        "def add_two_numbers(a: int, b: int) -> int:\n",
        "    return int(a) + int(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0E14OlG8hnR"
      },
      "source": [
        "Next, let's set up the system prompt and an initial user prompt/question for the agent/model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HAyBZ8jOP72"
      },
      "outputs": [],
      "source": [
        "# System prompt to inform the model about the tool is usage\n",
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are a helpful assistant. You can do math by calling a function 'add_two_numbers' if needed.\"\n",
        "}\n",
        "\n",
        "# A sample of user input asking a math question\n",
        "user_message = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"What is 90999999 + 10000001?\"\n",
        "}\n",
        "\n",
        "messages = [system_message, user_message]\n",
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiYh6Tuz8hnT"
      },
      "source": [
        "Ask the agent/model to respond."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBWJpDw9OP72"
      },
      "outputs": [],
      "source": [
        "# Ask llama3.2 to respond\n",
        "response = ollama.chat(\n",
        "    model='llama3.2:1b',\n",
        "    messages=messages,\n",
        "    tools=[add_two_numbers]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u1j0apmOP72"
      },
      "outputs": [],
      "source": [
        "response.message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4RDhuL3RZzV"
      },
      "outputs": [],
      "source": [
        "response.message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FOcIIRuOP73"
      },
      "outputs": [],
      "source": [
        "# Check if the model called a function\n",
        "if response.message.tool_calls:\n",
        "    for tool_call in response.message.tool_calls:\n",
        "        func_name = tool_call.function.name   # e.g., \"add_two_numbers\"\n",
        "        args = tool_call.function.arguments   # e.g., {\"a\": 10, \"b\": 10}\n",
        "        # If the function name matches and we have it in our tools, execute it:\n",
        "        if func_name == \"add_two_numbers\":\n",
        "            result = add_two_numbers(**args)\n",
        "            print(\"Function output:\", result)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od5Yxz0rOP73"
      },
      "source": [
        "---\n",
        "\n",
        "### Q1: Does the above output look correct? Does it look like the sum of the numbers 90999999 and 10000001? Why is it not correct?\n",
        "\n",
        "(Hint: there is nothing wrong with the model/agent here, but rather the tool implementation; namely, Python's [type hints](https://docs.python.org/3/library/typing.html) are not a guarantee that the correct/intended data type is used, so you may need to add some type casting inside of the function `add_two_numbers`)\n",
        "\n",
        "The original response concatinates the values together to be 0900000010000001. After the function's values has been type cast, the answer is 101000000.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XjJZptbOP73"
      },
      "outputs": [],
      "source": [
        "# Complete the agent's tool call and allow the model to use output to formulate an answer\n",
        "\"\"\" (Continuing from previous code) \"\"\"\n",
        "available_functions = {\"add_two_numbers\": add_two_numbers, \"multiply_two_numbers\": multiply_two_numbers}\n",
        "\n",
        "\"\"\" System prompt to inform the model about the tool is usage \"\"\"\n",
        "\n",
        "\"\"\" Model's initial response after possibly invoking the tool \"\"\"\n",
        "assistant_reply = response.message.content\n",
        "print(\"Assistant (initial):\", assistant_reply)\n",
        "\n",
        "\"\"\" If a tool was called, handle it \"\"\"\n",
        "for tool_call in (response.message.tool_calls or []):\n",
        "    func = available_functions.get(tool_call.function.name)\n",
        "    if func:\n",
        "        result = func(**tool_call.function.arguments)\n",
        "        # Provide the result back to the model in a follow-up message\n",
        "        messages.append({\"role\": \"assistant\", \"content\": f\"The result is {result}.\"})\n",
        "        messages.append({\"role\": \"user\", \"content\": \"Can you summarize and state the results you found?\"})\n",
        "        follow_up = ollama.chat(model='llama3.2:1b', messages=messages)\n",
        "        print(\"Assistant (final):\", follow_up.message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PtVEUyGU6Eg"
      },
      "source": [
        "---\n",
        "\n",
        "### Q2: Try running the code cell below. Does it return the expect result? If note, then add/modify the necessary code to allow Llama3.2 to use its  multiplication tool. Then rerun your code cell below; now did it output the expected result?\n",
        "\n",
        "The first response was the model saying it could not calculate the result because the function was incomplete. After fixing the function, it displayed a step by step process of how to multiply the numbers together.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIH7CWClVx9A"
      },
      "outputs": [],
      "source": [
        "# Implement a multiplication function by replacing the `pass` statement below with the correct return statement\n",
        "def multiply_two_numbers(a: int, b: int) -> int:\n",
        "    return int(a) * int(b)\n",
        "\n",
        "\n",
        "\"\"\" System prompt to inform the model about the tool is usage \"\"\"\n",
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are a helpful assistant. You can do addition by calling the function 'add_two_numbers' or multiplication by calling the function 'multiply_two_numbers'.\"\n",
        "}\n",
        "# User asks a question that involves a calculation\n",
        "user_message = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"What is 10001 times 6?\"\n",
        "}\n",
        "\n",
        "messages = [system_message, user_message]\n",
        "\n",
        "response = ollama.chat(\n",
        "    model='llama3.2:1b',\n",
        "    messages=messages,\n",
        "    tools=[add_two_numbers, multiply_two_numbers]  # pass the actual function object as a tool\n",
        ")\n",
        "\n",
        "# Model's initial reponse after (hopefully) calling the tool\n",
        "assistant_reply = response.message.content\n",
        "print(\"Assistant (initial):\", assistant_reply)\n",
        "\n",
        "# If a tool was called, then handle it\n",
        "available_functions = {\"add_two_numbers\": add_two_numbers, \"multiply_two_numbers\": multiply_two_numbers}\n",
        "for tool_call in (response.message.tool_calls or []):\n",
        "    func = available_functions.get(tool_call.function.name)\n",
        "    if func:\n",
        "        result = func(**tool_call.function.arguments)\n",
        "        # Provide the result back to the model in a follow-up message\n",
        "        messages.append({\"role\": \"assistant\", \"content\": f\"The result is {result}.\"})\n",
        "        messages.append({\"role\": \"user\", \"content\": \"Can you summarize and state the results you found?\"})\n",
        "        follow_up = ollama.chat(model='llama3.2:1b', messages=messages)\n",
        "        print(\"Assistant (final):\", follow_up.message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z9UtCF9WR8T"
      },
      "outputs": [],
      "source": [
        "follow_up.message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGXvIi78UANq"
      },
      "source": [
        "Next let's equip our agent to retrieve external information, which will require a few more tools to be able to search the web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM5x6koPOP73"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF86IHiuTjm3"
      },
      "outputs": [],
      "source": [
        "!pip install -U duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWwZ-tuOOP73"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "\n",
        "\n",
        "def search_web(query: str) -> str:\n",
        "  return DuckDuckGoSearchResults(backend=\"news\").run(query)\n",
        "\n",
        "tool_search_web = {'type':'function', 'function':{\n",
        "  'name': 'search_web',\n",
        "  'description': 'Search the web',\n",
        "  'parameters': {'type': 'object',\n",
        "                'required': ['query'],\n",
        "                'properties': {\n",
        "                    'query': {'type':'str', 'description':'the topic or subject to search on the web'},\n",
        "}}}}\n",
        "\n",
        "# Quickly test and see what a general web news search for Los Angeles yields\n",
        "search_web(query=\"Los Angeles\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgftifMXOP73"
      },
      "outputs": [],
      "source": [
        "def search_ys(query: str) -> str:\n",
        "  engine = DuckDuckGoSearchResults(backend=\"news\")\n",
        "  return engine.run(f\"site:sports.yahoo.com {query}\")\n",
        "\n",
        "tool_search_ys = {'type':'function', 'function':{\n",
        "  'name': 'search_ys',\n",
        "  'description': 'Search for sports news',\n",
        "  'parameters': {'type': 'object',\n",
        "                'required': ['query'],\n",
        "                'properties': {\n",
        "                    'query': {'type':'str', 'description':'the sport, sports team, or subject to search'},\n",
        "}}}}\n",
        "\n",
        "# Quickly test and see what a search for Los Angeles in the sports section of the news yields\n",
        "search_ys(query=\"Los Angeles\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiSvIqCDOP74"
      },
      "outputs": [],
      "source": [
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are a helpful assistant with access to tools for search the web for current news and events.\"\n",
        "    }\n",
        "user_message = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Tell me about sports in the city of Denver.\" # YOU WILL CHANGE THIS QUESTION, SEE Q3 BELOW\n",
        "}\n",
        "messages = [system_message, user_message]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_gc7WNWcCnu"
      },
      "outputs": [],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFUin1KZbF34"
      },
      "outputs": [],
      "source": [
        "response = ollama.chat(\n",
        "  model=\"llama3.2:1b\",\n",
        "  tools=[tool_search_web, tool_search_ys],\n",
        "  messages=messages\n",
        ")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba8XooQfcLX5"
      },
      "outputs": [],
      "source": [
        "# Model's initial reponse after (hopefully) calling the tool\n",
        "assistant_reply = response.message.content\n",
        "print(\"Assistant (initial):\", assistant_reply)\n",
        "\n",
        "# If a tool was called, then handle it\n",
        "available_functions = {'search_web':search_web, 'search_ys':search_ys}\n",
        "for tool_call in (response.message.tool_calls or []):\n",
        "    func = available_functions.get(tool_call.function.name)\n",
        "    if func:\n",
        "        result = func(**tool_call.function.arguments)\n",
        "        # Provide the result back to the model in a follow-up message\n",
        "        messages.append({\"role\": \"assistant\", \"content\": f\"The result is {result}.\"})\n",
        "        messages.append({\"role\": \"user\", \"content\": \"Can you summarize and state the results you found?\"})\n",
        "        follow_up = ollama.chat(model='llama3.2:1b', messages=messages)\n",
        "        print(\"Assistant (final):\", follow_up.message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3UxIzbCjOXz"
      },
      "source": [
        "---\n",
        "\n",
        "### Q3: The question above currently asks about Denver, but change the question to include a word or reference to sports. Does the agent use the correct tool based on your prompt/question? Be sure to also run the code cells above with your modified promp/question.\n",
        "\n",
        "The first response only mentioned broncos related events and after adding the word sport to it, it mentioned NWSL and the Denver nuggets. It does use the correct tools to search Duck Duck Go for recent news. Rewording the message does drastically change the output like putting sports before or after the city of denver part.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}